# -*- coding: utf-8 -*-
"""Project

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/12bL-y7oZWpVlcWeqmznlHR-Iq9DSns_j

# **EDA and Visualization**

**FIFA 22 Sport Predicitions**

In this project, when one is making the best team for their FIFA club, the capabilities of the players added are the most important. Do those players fill the hole in your formation? Are they the right fit for the team? Do they have the skills to back up their cost? Our project is about calculating the ratings of these players using the Attributes and Positions of the player to help anyone know and decide if the player is an excellent addition to their club.

## **Data Preprocessing and Cleaning**
"""

import pandas as pd
import numpy as np
from sklearn.metrics import mean_absolute_error, mean_squared_error
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import LabelEncoder
from sklearn.preprocessing import OneHotEncoder
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from xgboost import XGBRegressor
import pickle

"""Running player_22 files after testing the entire process with player_21 files"""

df=pd.read_csv('players_21.csv')
unseen_df = pd.read_csv('players_22.csv')

df.columns

"""Removing all useless columns"""

#Removing all columns with over 30% null values

# Calculate the threshold for 30% null values
threshold = len(df) * 0.3

# Use dropna with the thresh parameter
df.dropna(thresh=threshold, axis=1, inplace = True)
unseen_df.dropna(thresh=threshold, axis=1, inplace = True)

# Now df contains only columns with less than 30% null values

#Removing useless columns
useless_columns = ['sofifa_id', 'player_url', 'long_name',
                   'body_type', 'real_face',
                   'player_face_url', 'club_logo_url', 'nation_flag_url',
                    'wage_eur','nationality_id', 'nationality_name',
                   'club_jersey_number', 'club_joined', 'club_contract_valid_until', 'club_flag_url',"ls", "st","rs", "lw", "lf", "cf", "rf", "rw", "lam","cam","ram","lm", "lcm", "cm","rcm", "rm", "lwb", "ldm", "cdm", "rdm","rwb", "lb",
              "lcb", "cb", "rcb","rb","gk"
                   ]

new_df = df.drop(useless_columns, axis = 1)
unseen = unseen_df.drop(useless_columns, axis=1)

new_df.info()

"""## **Feature Engineering**

Encoding and Imputating Both Datasets
"""

# Impute missing values for numerical columns
numerical_imputer = SimpleImputer(strategy='mean')
#for players_21
df_imputed_numerical = pd.DataFrame(numerical_imputer.fit_transform(new_df.select_dtypes(include='number')), columns=new_df.select_dtypes(include='number').columns)
#For players_22
unseen_imputed_numerical = pd.DataFrame(numerical_imputer.fit_transform(unseen.select_dtypes(include='number')), columns=unseen.select_dtypes(include='number').columns)

# Impute missing values for categorical columns
for column in new_df.select_dtypes(include='object').columns:
    new_df[column] = new_df[column].fillna(new_df[column].mode()[0])
    unseen[column] = unseen[column].fillna(unseen[column].mode()[0])

# Now new_df contains the entire dataset with imputed values for both numerical and categorical columns

# Label encode categorical variables
label_encoder = LabelEncoder()

for column in new_df.select_dtypes(include='object').columns:
  #player_21
    new_df[column] = label_encoder.fit_transform(new_df[column])
  #players_22
    unseen[column] = label_encoder.fit_transform(unseen[column])


# Concatenate the encoded columns with the imputed numerical columns
df_final = pd.concat([df_imputed_numerical, new_df.select_dtypes(include='object')], axis=1)
unseen_final = pd.concat([unseen_imputed_numerical, unseen.select_dtypes(include='object')], axis=1)

# Now df_final contains the entire dataset with imputed values and label encoded categorical columns

df_final = df_final.dropna()
unseen_final = unseen_final.dropna()

"""Training models with Player_21"""

y = df_final["overall"]

X = df_final.drop(columns=['overall'])

from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

"""Creating and training the model using the RandomForest, XGBoost, and Gradient Boost Regressors that can predict a player rating."""

# RandomForestRegressor
rf_model = RandomForestRegressor(random_state=42)
rf_model.fit(X_train, y_train)
importances = rf_model.feature_importances_
indices = np.argsort(importances)[::-1]
print("Feature Ranking:")
for i in range(X.shape[1]):
  print(f"{i + 1}.Feature {X.columns[indices[i]]}({importances[indices[i]]})")

top_features = [X.columns[indices[i]] for i in range(30)]
print("\nTop 20 features:")
print(top_features)

"""After breaking down the features into how much they contribute to the calculation of player rating, we decided to use only the top 5: value, release clause, dob/age, potential, and movement reactions. When the values were added up, it proved 0.99, meaning all values below tthat are negible."""

#Changing our datasets to include only the top 5 features,
features = ['value_eur', 'release_clause_eur', 'age', 'potential', 'movement_reactions']
new_df = df_final[features]
unseen_y = unseen_final["overall"]
new_unseen = unseen_final[features]

"""We now have the values we will be using to train the other two models: GradientBoost and XGBoost Regressor"""

#Scaling the dataset
from sklearn.preprocessing import StandardScaler
X= new_df
sc = StandardScaler()
X=sc.fit_transform(X.copy())
new_unseen = sc.fit_transform(new_unseen.copy())

"""## **Training Models**

Splitting our modified Dataset and training with GradientBoostingRegressor and XGBRegressor.
"""

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# GradientBoostingRegressor
gb_model = GradientBoostingRegressor(random_state=42)
gb_model.fit(X_train, y_train)
gb_predictions = np.round(gb_model.predict(X_test)).ravel()
gb_rmse = np.sqrt(mean_absolute_error(y_test, gb_predictions))
print(f'Mean Absolute Error: {gb_rmse}')

# XGBRegressor
xgb_model = XGBRegressor(random_state=42)
xgb_model.fit(X_train,y_train)
xgb_predictions = xgb_model.predict(X_test)
xgb_rmse = np.sqrt(mean_absolute_error(y_test, xgb_predictions))
print(f'Mean Absolute Error: {xgb_rmse}')

"""With XGBoost Regressor, we cross-validate and proceed with it as the model of choice"""

from sklearn.model_selection import GridSearchCV

# Defining the parameter grid for hyperparameter tuning
param_grid = {
    'n_estimators': [50, 250, 500],
    'max_depth': [10, 20, 40],
    'min_samples_split': [17, 39, 100]
}

# Create XGBoost Regressor
xgb_mol = XGBRegressor()

# Create GridSearchCV object
grid_search = GridSearchCV(xgb_mol, param_grid, cv=5, scoring='neg_mean_squared_error', n_jobs=-1)

# Fit the model
grid_search.fit(X, y)

"""## **Evalutaion**

Testing the model's competence with unseen data

Unseen data=players_22 file data
"""

unseen_test = grid_search.predict(new_unseen)
rfg_unseen = np.sqrt(mean_absolute_error(unseen_test, unseen_y))
print(f'Mean Absolute Error: {rfg_unseen}')

"""Saving the Model"""

kk = pickle.dump(grid_search, open('model.pkl','wb'))
scaler_sumn = pickle.dump(sc, open("scaler_obj.pkl", "wb"))